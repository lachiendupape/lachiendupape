version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"   # Ollama HTTP API
    volumes:
      - ./ollama:/root/.ollama   # persists models
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    # For Docker Compose (non-swarm), use the shorthand:
    gpus: all

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      - ollama
    ports:
      - "3000:8080"     # Web UI -> http://localhost:3000
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./openwebui:/app/backend/data  # persists chats, settings

  # --- OPTIONAL: vLLM (OpenAI-compatible /v1 API) ---
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm
  #   restart: unless-stopped
  #   ports:
  #     - "8000:8000"   # OpenAI-compatible endpoint
  #   environment:
  #     - HF_TOKEN=${HF_TOKEN-}   # set in your .env if using gated models
  #   volumes:
  #     - ./vllm:/root/.cache/huggingface
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: ["gpu"]
  #   gpus: all
  #   command: >
  #     --model meta-llama/Meta-Llama-3.1-8B-Instruct
  #     --quantization awq
  #     --max-model-len 8192
